\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage[cache=true,outputdir=build]{minted}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{amsmath}
\input{../../macros}

\begin{document}
\begin{enumerate}

    \item
        The computational cost of the forward substitution algorithm, for a matrix of size $n \times n$,
        scales as~$\mathcal O(n)$.

    \item
        The computational cost to calculate the LU decomposition of an invertible matrix scales as $\mathcal O(n^2)$.

    \item
        Suppose that $\mat A$ is a symmetric, positive definite matrix.
        Calculating the Cholesky decomposition of~$\mat A$ requires fewer floating point operations than calculating the LU decomposition of~$\mat A$.

    \item
        The condition number~$\kappa_2(\mat A)$ of a square invertible matrix~$\mat A$ is always strictly larger than 1.

    \item
        The spectral radius of a matrix square~$\mat A$ is zero if and only if~$\mat A$ is zero.

    \item
        If the condition number of a matrix~$\mat A$ is very large ($\gg 1$),
        then it is possible that rounding errors arising from floating point arithmetic will have a large impact on 
        the accuracy of the numerical solution to the linear system~$\mat A \vect x = \vect b$
        (calculated by LU decomposition followed by forward and backward substitution, for example).

    \item
        Suppose that $\mat A \in \real^{n \times n}$ is symmetric and positive definite,
        and let $\vect b \in \real^n$.
        Consider the following iterative method for solving the linear system $\mat A \vect x = \vect b$:
        \begin{equation}
            \label{eq:richardson}
            \vect x^{(k+1)} = \vect x^{(k)} + \omega \left( \vect b - \mat A \vect x^{(k)} \right).
        \end{equation}
        This iteration converges to the exact solution of the linear system for all~$\omega \in \real$.

    \item
        The convergence speed of the iteration~\eqref{eq:richardson},
        for the optimal value of~$\omega$,
        is independent of the condition number~$\kappa_2(\mat A)$.

    \item
        If~$\mat A$ is symmetric positive definite,
        there always exists~$\omega \in \real$ such that
        the iteration~\eqref{eq:richardson} converges.

    \item
        If $\mat A$ is a nonzero matrix,
        then its norm $\norm{\mat A}_2$ is strictly positive.

    \item
        Suppose that we want to solve the linear systems~$\mat A \vect x = \vect b$ and $\mat A \vect x = \vect c$ by a direct method.
        To this end, the LU decomposition of~$\mat A$ can be calculated only once.

    \item
        Assuming that the matrix \julia{A} and the vector \julia{b} are already defined,
        write (on paper) Julia code implementing 100 iterations of~\eqref{eq:richardson}.
\end{enumerate}
\end{document}



