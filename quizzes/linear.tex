\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage[cache=true]{minted}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{amsmath}
\input{../../macros}

\begin{document}
\begin{enumerate}

    \item
        The computational cost of the forward substitution algorithm, for a matrix of size $n \times n$,
        scales as~$\mathcal O(n)$.

    \item
        The computational cost to calculate the LU decomposition of an invertible matrix scales as $\mathcal O(n^2)$.

    \item
        Suppose that $\mat A$ is a symmetric, positive definite matrix.
        Calculating the Cholesky decomposition of~$\mat A$ requires fewer floating point operations than calculating the LU decomposition of~$\mat A$.

    \item
        The condition number~$\kappa_2(\mat A)$ of a square invertible matrix~$\mat A$ is always strictly larger than 1.

    \item
        The spectral radius of a matrix square~$\mat A$ is zero if and only if~$\mat A$ is zero.

    \item
        If the condition number of a matrix~$\mat A$ is very large ($\gg 1$),
        then it is possible that rounding errors arising from floating point arithmetic will have a large impact on 
        the accuracy of the numerical solution to the linear system~$\mat A \vect x = \vect b$
        (calculated by LU decomposition followed by forward and backward substitution, for example).

    \item
        Suppose that $\mat A \in \real^{n \times n}$ is symmetric and positive definite,
        and let $\vect b \in \real^n$.
        Consider the following iterative method for solving the linear system $\mat A \vect x = \vect b$:
        \begin{equation}
            \label{eq:richardson}
            \vect x^{(k+1)} = \vect x^{(k)} + \omega \left( \vect b - \mat A \vect x^{(k)} \right).
        \end{equation}
        This iteration converges to the exact solution of the linear system for all~$\omega \in \real$.

    \item
        The convergence speed of the iteration~\eqref{eq:richardson},
        for the optimal value of~$\omega$,
        is independent of the condition number~$\kappa_2(\mat A)$.

    \item
        If~$\mat A$ is symmetric positive definite,
        there always exists~$\omega \in \real$ such that
        the iteration~\eqref{eq:richardson} converges.

    \item
        If $\mat A$ is a nonzero matrix,
        then its norm $\norm{\mat A}_2$ is strictly positive.

    \item
        Suppose that we want to solve the linear systems~$\mat A \vect x = \vect b$ and $\mat A \vect x = \vect c$ by a direct method.
        To this end, the LU decomposition of~$\mat A$ can be calculated only once.

    \item
        Assuming that the matrix \julia{A} and the vector \julia{b} are already defined,
        write (on paper) Julia code implementing 100 iterations of~\eqref{eq:richardson}.
\end{enumerate}

\newpage
\subsection*{Solutions}%
\begin{enumerate}
    \item 
        False. The computational cost scales as~$\mathcal O(n^2)$.

    \item
        False. The computational cost scales as~$\frac{2}{3} n^3 + \mathcal O(n^2)$ in general.

    \item
        True. The computational cost of Cholesky decomposition is~$\frac{1}{3}n^3 + \mathcal O(n^2)$.

    \item
        False. The condition number is always greater than \emph{or equal to} 1.
        For the identity matrix, the condition number is 1.

    \item
        False. 
        For example, the only eigenvalue of the nonzero matrix~\julia{A = [0 1; 0 0]} is 0,
        so its spectral radius is~0.

    \item
        True.

    \item
        False. For example, for~$\omega = 0$, it holds that $\vect x^{(k)} = \vect x^{(0)}$ for all~$k$.
        Therefore, unless $\vect x^{(0)}$ coincides with the solution of the linear system,
        the iteration does not converge to the solution.

    \item
        False. We proved in class that the larger the condition number, the slower the convergence.

    \item
        True.

    \item
        True. If this was false, then $\lVert \, \cdot \, \rVert_2$ would not be called a norm.

    \item
        True. This is the advantage of calculating the LU decomposition,
        compared to Gaussian elimination with corresponding operations on the right-hand side.

    \item
        For example:
        \begin{minted}{julia}
    A = [3 1; 1 3]
    b = [1, 1]
    w = .01
    x = [0, 0]  # Initial guess
    for i in 1:100
        x += w * (b - A*x)
    end
        \end{minted}
\end{enumerate}

\end{document}



